{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../key\") as fs:\n",
    "    os.environ['OPENAI_API_KEY'] = fs.readlines()[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the KG\n",
    "from gpt_index import SimpleDirectoryReader, LLMPredictor\n",
    "from gpt_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\n",
    "from langchain import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LLM\n",
    "# NOTE: at the time of demo, text-davinci-002 did not have rate-limit errors\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 24787 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# NOTE: can take a while! \n",
    "index = GPTKnowledgeGraphIndex(\n",
    "    documents, \n",
    "    chunk_size_limit=512, \n",
    "    max_triplets_per_chunk=2,\n",
    "    llm_predictor=llm_predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the index kg\n",
    "index.save_to_disk('index_kg.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the new index\n",
    "new_index = GPTKnowledgeGraphIndex.load_from_disk('index_kg.json', llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: Tell me more about Interleaf\n",
      "INFO:root:> Query keywords: ['software', 'Interleaf', 'company', 'history']\n",
      "INFO:root:> Extracted relationships: The following are knowledge triplets in the form of (subset, predicate, object):\n",
      "('software', 'generate web sites for', 'galleries')\n",
      "('Interleaf', 'got crushed by', \"Moore's Law\")\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 305 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# Querying the KG\n",
    "response = new_index.query(\n",
    "    \"Tell me more about Interleaf\", \n",
    "    include_text=False, \n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Interleaf was a software company that was acquired by Quark, Inc. in 2000. Interleaf produced software that generated web sites for galleries and other businesses.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"example.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x14ec73d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Visualize the graph\n",
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = new_index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"example.html\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59fb295b6d1575fb24ec56423f3d37055060f3ea09119d40a38b40c3288abf36"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
